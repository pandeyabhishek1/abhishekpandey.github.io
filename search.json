[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Abhishek Pandey",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "sub1.html",
    "href": "sub1.html",
    "title": "Untitled",
    "section": "",
    "text": "x &lt;- rnorm(50)\ny &lt;- rnorm(50)\nplot(x, y, main = \"Scatter Plot\", pch = 19)"
  },
  {
    "objectID": "Assignment1.html",
    "href": "Assignment1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Question 1. Try Anscombe’s examples (anscombe01.R on Teams)\nHere is the output chart’s from Anscombe’s examples.\n     \nQuestion 2. Google “generative art”. Cite some examples.\nHere are some of the generative art.\n\n\n\nGenerative Architecture by Michael Hansmeyer\n\n\n\n\n\nMuqarnas by Michael Hansmeyer\n\n\n\n\n\nPlatonic Solids by Michael Hansmeyer\n\n\n\n\n\nSprawl by Mark J Stock\n\n\nQuestion 3: Run Fall.R (on Teams) a. Give your own colors (e.g. Winter). b. Export the file and post on your GitHub website.\n\n\n\nDark Green Coloured Generative Art Tree\n\n\nQuestion 4: Write a critique on a chart in published work (book/article/news website) (Hint: Learn from Nathan Yau’s example discussed in class). Post on your website.\n\n\n\nGNH index by gender chart from World Happiness Report\n\n\nThe title “GNH index by gender” is clear and informative. However, the chart lacks a clear context beyond the brief title. There’s no accompanying text or annotation to explain why these differences in GNH index exist or why they are significant. The colors chosen (pink for Female, orange for National, blue for Male) are distinct and help differentiate the categories. However, the chart would benefit from a more standardized color scheme, especially one that is colorblind-friendly.\nThe use of a 3D bar chart distorts the perception of the data. The 3D perspective can make it difficult to accurately compare the heights of the bars, especially since the tops of the bars are not aligned with the background gridlines. This visual distortion can mislead viewers about the actual differences between the GNH indices. A 2D bar chart or even a simple line graph would likely be more effective for this type of data. The 3D design adds unnecessary complexity without providing additional insight.\nWhile the chart does not appear to intentionally mislead, notice the axis that starts at 0.66. The graph shouldn’t have that because length is the visual cue here, and it makes the differences look greater than they are. In addition, there’s no visible citation or reference to the source of the data. Including the data source would strengthen the chart’s credibility.\nA simple 2D bar chart would improve readability and accuracy, allowing viewers to more easily compare the GNH indices. Also, adding a citation for the data source would enhance the chart’s credibility and transparency. Furthermore, using colors that are distinguishable to those with color vision deficiencies, and ensure that all essential information is accessible without relying solely on color."
  },
  {
    "objectID": "Class5.html",
    "href": "Class5.html",
    "title": "Class-5",
    "section": "",
    "text": "Big data analysis, while offering immense potential, comes with significant challenges that can undermine its effectiveness if not properly addressed. One of the primary pitfalls is the quality of data. Large datasets often contain inaccuracies, inconsistencies, missing values, or outdated information, which can distort the results of analysis if not meticulously cleaned and validated. This can lead to erroneous insights or misguided decisions, especially if analysts assume that the sheer volume of data equates to accuracy. Additionally, integrating data from disparate sources can introduce further complexity, as different systems and formats may not align seamlessly. Without rigorous data governance practices, the analysis may be compromised by poor data quality, rendering any conclusions less reliable.\nAnother key challenge in big data analysis is the risk of overfitting and misinterpretation. Overfitting occurs when models become overly complex, tailoring themselves too closely to the specific dataset used for training, which reduces their applicability to new or broader data. This can result in predictions that perform well in controlled environments but fail in real-world situations. Moreover, the complexity of big data can overwhelm analysts, increasing the risk of misinterpreting trends or relationships within the data. Over-reliance on algorithms and quantitative methods can also lead to ignoring important qualitative or contextual factors that may not be easily captured in the data but are critical for decision-making. These challenges highlight the need for not only advanced analytical tools but also skilled interpretation to avoid drawing inaccurate or biased conclusions from big data.\n\n\n\nOverfitting is a common challenge in data analysis and machine learning, occurring when a model becomes too complex and overly tailored to the specific dataset it is trained on. This happens when the model captures not only the underlying patterns but also the noise and random fluctuations in the data. As a result, while the model may perform exceptionally well on the training data, it struggles to generalize to new, unseen data, leading to poor predictive performance in real-world applications. Overfitting often arises when a model is exposed to too many variables or features, and this excess complexity allows it to memorize the data rather than learning its fundamental structure. Techniques such as cross-validation, regularization, and simplifying the model can help mitigate the risk of overfitting by ensuring that it captures only the most relevant patterns.\nClosely related to overfitting is the issue of overparameterization, which occurs when a model contains too many parameters relative to the amount of data available. In overparameterized models, the model’s flexibility increases, allowing it to fit the training data almost perfectly, even if the data contains noise or anomalies. However, this can lead to models that are excessively sensitive to small changes in the data, resulting in poor generalization and unstable predictions. Overparameterization is particularly problematic in deep learning and neural networks, where the number of parameters can easily exceed the amount of meaningful data available for training. To avoid overparameterization, it is crucial to balance model complexity with the size and quality of the dataset, using techniques like dimensionality reduction, pruning, and regularization to ensure that the model remains robust and generalizable."
  },
  {
    "objectID": "Class5.html#short-notes-on",
    "href": "Class5.html#short-notes-on",
    "title": "Class-5",
    "section": "",
    "text": "Big data analysis, while offering immense potential, comes with significant challenges that can undermine its effectiveness if not properly addressed. One of the primary pitfalls is the quality of data. Large datasets often contain inaccuracies, inconsistencies, missing values, or outdated information, which can distort the results of analysis if not meticulously cleaned and validated. This can lead to erroneous insights or misguided decisions, especially if analysts assume that the sheer volume of data equates to accuracy. Additionally, integrating data from disparate sources can introduce further complexity, as different systems and formats may not align seamlessly. Without rigorous data governance practices, the analysis may be compromised by poor data quality, rendering any conclusions less reliable.\nAnother key challenge in big data analysis is the risk of overfitting and misinterpretation. Overfitting occurs when models become overly complex, tailoring themselves too closely to the specific dataset used for training, which reduces their applicability to new or broader data. This can result in predictions that perform well in controlled environments but fail in real-world situations. Moreover, the complexity of big data can overwhelm analysts, increasing the risk of misinterpreting trends or relationships within the data. Over-reliance on algorithms and quantitative methods can also lead to ignoring important qualitative or contextual factors that may not be easily captured in the data but are critical for decision-making. These challenges highlight the need for not only advanced analytical tools but also skilled interpretation to avoid drawing inaccurate or biased conclusions from big data.\n\n\n\nOverfitting is a common challenge in data analysis and machine learning, occurring when a model becomes too complex and overly tailored to the specific dataset it is trained on. This happens when the model captures not only the underlying patterns but also the noise and random fluctuations in the data. As a result, while the model may perform exceptionally well on the training data, it struggles to generalize to new, unseen data, leading to poor predictive performance in real-world applications. Overfitting often arises when a model is exposed to too many variables or features, and this excess complexity allows it to memorize the data rather than learning its fundamental structure. Techniques such as cross-validation, regularization, and simplifying the model can help mitigate the risk of overfitting by ensuring that it captures only the most relevant patterns.\nClosely related to overfitting is the issue of overparameterization, which occurs when a model contains too many parameters relative to the amount of data available. In overparameterized models, the model’s flexibility increases, allowing it to fit the training data almost perfectly, even if the data contains noise or anomalies. However, this can lead to models that are excessively sensitive to small changes in the data, resulting in poor generalization and unstable predictions. Overparameterization is particularly problematic in deep learning and neural networks, where the number of parameters can easily exceed the amount of meaningful data available for training. To avoid overparameterization, it is crucial to balance model complexity with the size and quality of the dataset, using techniques like dimensionality reduction, pruning, and regularization to ensure that the model remains robust and generalizable."
  },
  {
    "objectID": "Class5.html#name-the-technologiestechniques-wickham-introduced.-what-are-his-main-points.-summarize-and-comment.",
    "href": "Class5.html#name-the-technologiestechniques-wickham-introduced.-what-are-his-main-points.-summarize-and-comment.",
    "title": "Class-5",
    "section": "Name the technologies/techniques Wickham introduced. What are his main points. Summarize and comment.",
    "text": "Name the technologies/techniques Wickham introduced. What are his main points. Summarize and comment.\nHadley Wickham, a key figure in the data science community, is known for his contributions to data visualization, particularly with technologies and techniques like ggplot2, tidyverse, and dplyr in R. His work emphasizes the importance of tidy data, reproducibility, and the systematic transformation of raw data into meaningful visualizations. Wickham advocates for using layered grammar in data visualization, which allows users to build complex plots step by step. His main points likely center on simplifying data handling and making data science more accessible through intuitive, efficient tools.\nThese tools have revolutionized the way data is analyzed and visualized, especially by making complex statistical models easier to understand. Wickham’s focus on usability and reproducibility ensures that even users with limited programming backgrounds can engage deeply with their data, enabling wider participation in data-driven fields."
  },
  {
    "objectID": "Assignment4.html",
    "href": "Assignment4.html",
    "title": "Assignment 4",
    "section": "",
    "text": "##Assignment4Compiled\n\n\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\n\nfile_path &lt;- \"C:/Users/pande/Downloads/DV_ProjectData.csv\"  # or \"C:\\\\Users\\\\ams190002\\\\Downloads\\\\DV_ProjectData.csv\"\ndata &lt;- read.csv(file_path)\nhead(data)\nsummary(data)\n\n## Cleaning Data \n\nstate_means &lt;- data %&gt;%\ngroup_by(State) %&gt;%\nsummarise(across(c(VoterTurnoutRate, MedianHouseholdIncome, UnemploymentRate, HSCompletion, SomeCollegeEducation,PoorOrFairHealth, AvgPoorPhysHealthDays, AvgPoorMentalHealthDays, ObesityRate, \nFoodEnviornmentIndex, FoodInsecurity, LimitedAccesstoHealthyFoods, PhysInactivityRate,Population), mean))\nView(state_means)\nstates_of_interest &lt;- c(\"CA\", \"AR\", \"TX\", \"OK\", \"NJ\")\nstate_means_filtered &lt;- state_means %&gt;%\n  filter(State %in% states_of_interest)\n\npar(family=\"serif\")\n\n## Graph 1\n\nbluecolorramp &lt;- colorRampPalette(c(\"skyblue\", \"mediumblue\"))(length(state_means_filtered$MedianHouseholdIncome))\ncolors &lt;- bluecolorramp[rank(state_means_filtered$MedianHouseholdIncome)]\n\npar(family=\"serif\", cex=0.9, mar=c(3, 3.5, 3, 1))\nbarplot(state_means_filtered$VoterTurnoutRate, width=state_means_filtered$MedianHouseholdIncome, space=0, \n        col=colors, ylim=c(0,0.8), names.arg=state_means_filtered$State)\ntitle(main=\"Voter Turnout vs Median Household Income\", cex.main=1.5)\nmtext(\"Voter Turnout Rate\", , side=2, line=2.2, cex=1.1)\nbox(bty=\"l\")\nlegend(\"topright\", legend = c(\"Low Income\", \"High Income\"), fill = c(\"skyblue\", \"mediumblue\"), \n       title = \"Income Level\")\n\n## Graph 2\n\nreduceddata &lt;- data%&gt;%\n  filter(State == \"CA\"| State==\"AR\"|State ==\"TX\" | State == \"OK\"|State ==\"NJ\")\n\nggplot(data = reduceddata,aes(x=VoterTurnoutRate, color = State))+\n  geom_density()+\n  scale_color_brewer(palette = \"Dark2\")+\n  theme_minimal()+\n  theme(legend.position = \"none\",\n        text = element_text(family = \"serif\"))+\n  facet_wrap(~State)\n\n\n## Graph 3\n\n\nggplot(state_means_filtered, aes(x = reorder(State, VoterTurnoutRate), y = VoterTurnoutRate, fill= \"forestgreen\")) + \n  geom_col(show.legend=FALSE) + \n  labs(title = \"Voter Turnout Rate by State\", x = \"State\", y = \"Voter Turnout Rate\") + \n  theme_classic() +\n  theme(text = element_text(family = \"serif\")) +\n  coord_flip()\n\n##Graph 4\n\n# Cleaning Data to Include DFW Counties\ndfw_counties &lt;- c(\"Collin County\", \"Dallas County\", \"Denton County\", \"Ellis County\", \n                  \"Hunt County\", \"Kaufman County\", \"Rockwall County\", \n                  \"Johnson County\", \"Parker County\", \"Tarrant County\", \"Wise County\")\n\ndfw_data &lt;- subset(data, CountyName %in% dfw_counties)\n\n# Convert the data from wide to long format\ndfw_data_long &lt;- dfw_data %&gt;%\n  pivot_longer(cols = c(\"HSCompletion\", \"SomeCollegeEducation\"), \n               names_to = \"EducationLevel\", \n               values_to = \"Percentage\")\n\n# Create the column chart\nggplot(dfw_data_long, aes(x = CountyName, y = Percentage * 100, fill = EducationLevel)) + \n  geom_bar(stat = 'identity', position = 'dodge') + \n  labs(title = 'High School Completion vs Some College Education in DFW Metropolis ',\n       x = 'County Name', y = 'Percentage (%)') + \n  scale_fill_manual(name = 'Education Level', \n                    values = c('HSCompletion' = 'steelblue', 'SomeCollegeEducation' = 'forestgreen')) + \n  theme_minimal() + \n  theme(text = element_text(family = \"serif\"),\n        axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))"
  }
]